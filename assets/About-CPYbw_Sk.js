import{d as e,b as n,h as t,e as a,n as i,g as s,t as o,m as r,F as l,q as d,f as c,u as m,r as u,v as p,I as g,l as h}from"./vendor-CWSGjIKd.js";import{_ as b}from"./Modal.vue_vue_type_script_setup_true_lang-CB4rGY4A.js";import"./index-CNsawZkM.js";const y={"data-testid":"introduction",class:"w-full max-w-3xl text-center mb-10 pt-4","aria-labelledby":"about-heading"},f=e({__name:"Introduction",setup:e=>(e,i)=>(t(),n("section",y,[...i[0]||(i[0]=[a("h1",{id:"about-heading",class:"text-4xl font-bold mb-4"}," About Me ",-1),a("p",{class:"text-lg"}," I'm a backend software engineer specializing in platform and site reliability engineering in AI/ML, media streaming, and cloud infrastructure. I specialize in building high-quality user experiences using modern web technologies, and highly scalable, reliable cloud software for large, global platforms. ",-1)])]))}),v=["data-testid"],w={class:"bg-base-200 rounded-xl p-6 shadow-md hover:shadow-lg transition-all duration-300 border-l-4 border-primary/30 group-hover:border-primary/50 ml-2 md:ml-0"},k={class:"inline-block font-mono text-sm font-semibold text-base-content/70 mb-2 px-3 py-1 bg-base-300 rounded-full"},x={class:"text-xl font-bold text-base-content mb-2 mt-3"},I={key:0,class:"text-base font-semibold text-base-content/80 mb-3 flex items-center gap-2"},S={role:"article",class:"text-base-content/70 leading-relaxed"},D=e({__name:"CareerTimelineItem",props:{date:{},title:{},subtitle:{},description:{},position:{},testId:{default:"timeline-item"}},setup:e=>(l,d)=>(t(),n("li",{"data-testid":e.testId,tabindex:"0",role:"listitem",class:"group"},[d[1]||(d[1]=a("div",{class:"timeline-middle"},[a("div",{class:"flex items-center justify-center w-10 h-10 rounded-full bg-primary/20 text-primary shadow-md transition-all duration-300 group-hover:scale-110 group-hover:bg-primary/30 mx-2"},[a("svg",{"data-testid":"decorative-icon","aria-hidden":"true",class:"h-5 w-5",fill:"currentColor",viewBox:"0 0 20 20",xmlns:"http://www.w3.org/2000/svg"},[a("path",{"fill-rule":"evenodd",d:"M10 18a8 8 0 100-16 8 8 0 000 16zm3.857-9.809a.75.75 0 00-1.214-.882l-3.483 4.79-1.88-1.88a.75.75 0 10-1.06 1.061l2.5 2.5a.75.75 0 001.137-.089l4-5.5z","clip-rule":"evenodd"})])])],-1)),a("div",{class:i(`timeline-${e.position} mb-10 md:text-${"start"===e.position?"end":"start"}`)},[a("div",w,[a("time",k,o(e.date),1),a("h3",x,o(e.title),1),e.subtitle?(t(),n("p",I,[d[0]||(d[0]=a("svg",{class:"w-4 h-4 text-primary/60",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",xmlns:"http://www.w3.org/2000/svg"},[a("path",{"stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"2",d:"M19 21V5a2 2 0 00-2-2H7a2 2 0 00-2 2v16m14 0h2m-2 0h-5m-9 0H3m2 0h5M9 7h1m-1 4h1m4-4h1m-1 4h1m-5 10v-5a1 1 0 011-1h2a1 1 0 011 1v5m-4 0h4"})],-1)),r(" "+o(e.subtitle),1)])):s("",!0),a("p",S,o(e.description),1)])],2),d[2]||(d[2]=a("hr",{class:"border-base-300"},null,-1))],8,v))}),M={class:"w-full max-w-5xl mb-10 p-6 md:p-8 bg-base-100 rounded-xl shadow-xl border border-base-300"},C={class:"timeline timeline-snap-icon max-md:timeline-compact timeline-vertical","aria-label":"Career Timeline"},A=e({__name:"CareerTimeline",props:{timelineData:{}},setup(e){const i=e.timelineData||[{date:"2015 - Present",title:"Media Systems and Streaming Services",subtitle:"Sony Pictures, Disney Studios, HPE Services, and Rackspace",description:"Developed state-of-the-art video streaming experiences for millions of concurrent users and built AI/ML-driven film production systems using cloud services and MLOps across Fortune 500 entertainment companies.",position:"start"},{date:"2011 - 2014",title:"Web Development",subtitle:"RWI Studios",description:"Built and maintained client web apps, introducing early CI/CD and hosting automation practices that laid groundwork for future DevOps work.",position:"end"},{date:"2009 - 2011",title:"Internet Network Engineering",subtitle:"Frontier Internet",description:"Built and supported ISP infrastructure, improving uptime and helping modernize network configuration processes.",position:"start"},{date:"2004 - 2007",title:"Linux Server Administration",subtitle:"TigerDirect",description:"Managed servers and internal systems, hardening security and introducing early automation tools.",position:"end"}];return(e,s)=>(t(),n("div",M,[s[0]||(s[0]=a("div",{class:"text-center mb-8"},[a("h2",{class:"text-4xl font-bold text-base-content mb-2"},"Career Timeline"),a("p",{class:"text-base-content/70 text-lg"},"Professional journey and milestones")],-1)),a("ul",C,[(t(!0),n(l,null,d(m(i),((e,n)=>(t(),c(D,{key:n,date:e.date,title:e.title,subtitle:e.subtitle,description:e.description,position:e.position},null,8,["date","title","subtitle","description","position"])))),128))])]))}}),T={class:"w-full max-w-5xl mb-10 p-6 bg-base-100 rounded-lg shadow-lg"},z={class:"grid grid-cols-2 sm:grid-cols-3 gap-6"},P={class:"font-semibold mb-2"},F=["onClick"],L=["innerHTML"],W=e({__name:"Skills",setup(e){const i=u(null),r=u(!1),y=u(!0),f=u(0),v=[{id:"software-engineering-modal",category:"Software Engineering",iconName:"fa6-solid:code",content:'\n      <p class="leading-relaxed mb-6">\n        My journey in software engineering started well before it became my profession. Back in the early days of the web, I was inspired by my love for video games and anime, which led me to build my first websites. In 1996, I was using tools like Microsoft FrontPage, Netscape Composer, GeoCities, and Tripod to experiment with web development. By high school in 2002, I had upgraded to Macromedia Dreamweaver, which really solidified my interest in coding and systems design. It wasn\'t just a hobby anymore – it was something I knew I wanted to do long-term.\n      </p>\n      <p class="leading-relaxed mb-6">\n        Over the years, my career has evolved into full-stack development across different industries, always with a focus on creating systems that are scalable, maintainable, and high-performance. I\'m a big advocate of domain-driven design (DDD), which helps me align technical implementations with the real needs of the business. This approach lets me build around specific business logic, encapsulating it within bounded contexts to keep everything clean and organized.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Frontend Development</h4>\n      <p class="leading-relaxed mb-6">\n        When it comes to frontend development, I prefer Vue because it allows me to build reusable, composable components that simplify development, especially in larger applications. For bundling, I like Vite for its fast Hot Module Replacement (HMR) and simple setup, which is particularly useful in TypeScript projects where real-time feedback is critical.\n      </p>\n      <p class="leading-relaxed mb-6">\n        For styling, I often combine Tailwind CSS with DaisyUI. Tailwind\'s utility-first approach lets me quickly prototype UI elements without diving into custom CSS, while DaisyUI provides accessible, pre-styled components that ensure a consistent look and feel across the application.\n      </p>\n      <p class="leading-relaxed mb-6">\n        Accessibility is a priority for me because it\'s simply a non-negotiable part of modern web design. I make sure every interface works seamlessly across devices and assistive technologies by implementing ARIA roles, keyboard navigation, and screen reader support. I also bake in internationalization (i18n) from the start, using tools like i18next to make localization easy for global users.\n      </p>\n      <p class="leading-relaxed mb-6">\n        For testing, I go with Vitest for unit and integration tests, especially given its fast performance and native integration with Vite. I also use Vue Test Utils to verify that my components behave as expected. On top of that, DataDog RUM and Google Analytics help me track real user interactions, which I use to continuously improve both the user experience and performance.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Backend Development</h4>\n      <p class="leading-relaxed mb-6">\n        On the backend, I focus on building scalable microservices using the right language and framework for the job. Often, I work in TypeScript because many of my projects integrate closely with frontend services, and TypeScript\'s strict type safety helps minimize runtime errors while keeping development fast and maintainable.\n      </p>\n      <p class="leading-relaxed mb-6">\n        For high-performance systems – whether it\'s a gRPC or REST API, media transcoding pipeline, or a real-time application – I\'ll opt for Go. It handles concurrency really well and performs efficiently under heavy loads. If I\'m dealing with more flexible workflows, like data pipelines or machine learning tasks, I lean toward Python because its vast ecosystem and tools like Apache Airflow and Spark make it easy to work with large datasets or complex processes.\n      </p>\n      <p class="leading-relaxed mb-6">\n        Data handling is something I take seriously, and I use both SQL and NoSQL databases depending on the project. I tend to choose PostgreSQL or DynamoDB based on the data model and use Redis for caching. I often integrate with messaging systems like SQS or Kafka for handling async workflows.\n      </p>\n      <p class="leading-relaxed mb-6">\n        I like to follow clean architecture principles, focusing on modularity and separation of concerns. This ensures that the codebase stays maintainable and flexible as new features or requirements come up. To monitor everything, I use DataDog and Prometheus for real-time insights into service performance, capturing key metrics like response times, memory usage, and error rates. This helps me spot bottlenecks early and optimize proactively.\n      </p>\n      <h4 class="font-bold text-lg mb-3">DevOps</h4>\n      <p class="leading-relaxed mb-6">\n        I\'ve been working with various version control and CI/CD platforms since 2016, and GitHub and GitLab have become my preferred tools for managing code and automating workflows. I build CI/CD pipelines that cover everything—from linting and testing to deployment and monitoring. I also integrate security scanning tools like Snyk and Veracode to identify vulnerabilities early on.\n      </p>\n      <p class="leading-relaxed mb-6">\n        For release automation, I use Semantic Release with Conventional Commits to ensure that versioning is consistent and releases are automated without any hiccups.\n      </p>\n      <p class="leading-relaxed mb-6">\n        Containerization is a big part of my workflow. I use Docker for local development, especially with LocalStack when building AWS-based services. In production, Kubernetes is my go-to for orchestration, particularly when paired with ArgoCD, Argo Rollouts, Helm, and Istio.\n      </p>\n      <p class="leading-relaxed mb-6">\n        I place a strong emphasis on observability. DataDog is my preferred tool for centralized logs, metrics, and traces because it gives me real-time insights into how systems are performing and how users are interacting with services. DataDog\'s dashboards and alerting capabilities make it easy to track key metrics and quickly resolve any issues before they impact users. I also integrate OpsGenie or PagerDuty for incident management, making sure that issues are addressed efficiently.\n      </p>\n    '},{id:"cloud-infrastructure-modal",category:"Cloud Infrastructure",iconName:"fa6-solid:cloud",content:'\n      <p class="leading-relaxed mb-6">\n        My experience with cloud infrastructure has evolved from managing complex hybrid systems to designing multi-cloud solutions for large-scale production workloads. I\'ve spent a lot of time working with AWS, Azure, and GCP, always looking for ways to leverage cloud-native services to improve scalability, security, and cost efficiency.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Infrastructure as Code (IaC)</h4>\n      <p class="leading-relaxed mb-6">\n        Using tools like Pulumi, Terraform, and Serverless Framework, my preferred approach to IaC emphasizes using pre-built modules from within the service repository — this way, the infrastructure can scale quickly while still maintaining high security and performance standards. The goal is to ensure the infrastructure can evolve alongside the application and business requirements without introducing complexity or risk.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Cloud Infrastructure Monitoring</h4>\n      <p class="leading-relaxed mb-6">\n        I prefer DataDog and cloud-native solutions to track infrastructure performance and reliability. With DataDog, custom dashboards and alerts help me keep an eye on key metrics like resource utilization, latency, and network traffic so I can detect and fix issues proactively before they turn into bigger problems, which is critical for minimizing downtime. I also tie these monitoring tools into automated incident response workflows with OpsGenie or PagerDuty to make sure any issues are handled quickly and efficiently.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Networking</h4>\n      <p class="leading-relaxed mb-6">\n        I\'ve worked a lot with VPC architectures to ensure secure and efficient connectivity between services. This has included setting up secure networking solutions like DirectConnect, VPNs, and fine-tuning IAM roles to make sure resources are both secure and accessible only to authorized users. I also make sure that network traffic and security events are continuously monitored, so any threats can be detected and responded to before they become an issue.\n      </p>\n    '},{id:"media-engineering-modal",category:"Media Engineering",iconName:"fa6-solid:video",content:'\n      <p class="leading-relaxed mb-6">\n        Media engineering has been a core part of my work, especially with platforms like Disney+ and Crunchyroll. My focus has been on building and optimizing media transcoding, delivery, and playback to ensure video content is processed and delivered efficiently, no matter the device.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Transcoding</h4>\n      <p class="leading-relaxed mb-6">\n        I\'ve built and fine-tuned media transcoding pipelines using tools like FFmpeg and cloud services such as AWS Elemental MediaConvert, automating transcoding tasks and ensuring efficient delivery across a wide range of devices. Maintaining quality is a big priority for me — I work to ensure videos maintain high resolution and perform well, even when network conditions aren\'t ideal.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Delivery</h4>\n      <p class="leading-relaxed mb-6">\n        Smooth, efficient media delivery is essential for a great user experience. I\'ve worked extensively with adaptive bitrate streaming technologies like HLS and DASH to make sure content is optimized for different devices and network conditions. I\'ve also built and integrated Content Delivery Networks (CDNs) on Akamai, Fastly, and AWS CloudFront to reduce latency and offload traffic, ensuring that users can access content quickly and reliably, no matter where they\'re located.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Playback</h4>\n      <p class="leading-relaxed mb-6">\n        For me, optimizing video playback is all about giving users the best experience possible, no matter what device they\'re on or what their network situation looks like. I\'ve implemented adaptive bitrate (ABR) streaming, which adjusts video quality dynamically based on available bandwidth, providing a smoother, more consistent viewing experience even when network conditions change. I constantly monitor playback performance metrics—things like buffering events and user feedback—to proactively address any issues and ensure that playback remains high-quality at all times.\n      </p>\n    '},{id:"machine-learning-modal",category:"Machine Learning",iconName:"fa6-solid:brain",content:'\n      <p class="leading-relaxed mb-6">\n        My work in machine learning focuses on solving practical problems like video analysis, recommendation systems, film production, and automation through AI/ML models. Over the years, I\'ve collaborated with research teams to take models from development all the way to production, using frameworks like TensorFlow and PyTorch. I\'ve built end-to-end machine learning pipelines, managed model lifecycles, and deployed scalable solutions that handle real-time data processing.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Frameworks & Tools</h4>\n      <p class="leading-relaxed mb-6">\n        I\'ve primarily worked with computer vision models like U-Net, GroundingDINO, YOLO, Segment Anything, and VMAF. For training and deploying these models, I use TensorFlow and PyTorch. Whether it\'s for video analysis, film production, or content recommendation, I ensure the models are optimized for performance and scalability. In addition to using AWS SageMaker for model management, I also work with Weights & Biases and MLflow for experiment tracking and model lifecycle management. For monitoring system-level performance, infrastructure health, and model drift, I lean on DataDog and SageMaker Model Monitor to ensure models stay reliable and continue to meet evolving business needs.\n      </p>\n      <h4 class="font-bold text-lg mb-3">MLOps & Deployment</h4>\n      <p class="leading-relaxed mb-6">\n        Managing the full lifecycle of machine learning models requires a solid MLOps strategy. I lean on tools like MLflow, Weights & Biases, Kubernetes, and AWS SageMaker to automate and streamline processes from model training to evaluation and deployment. By setting up robust pipelines, I ensure that models are continuously monitored and retrained as new data becomes available, keeping them adaptable and effective.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Data Pipelines for ML</h4>\n      <p class="leading-relaxed mb-6">\n        Building reliable data pipelines is essential to any machine learning project. I work with tools like Apache Kafka, Spark, and Airflow to maintain efficient, real-time data flows that feed into the models. This ensures both training and production data are processed accurately and on time. Reliable, high-performing data pipelines are key to supporting real-time analytics and decision-making, and I continuously monitor them to ensure seamless operation across different environments.\n      </p>\n    '},{id:"data-engineering-modal",category:"Data Engineering",iconName:"fa6-solid:database",content:'\n      <p class="leading-relaxed mb-6">\n        Data engineering has always been a critical part of my work, especially when it comes to building scalable ETL pipelines that support real-time data processing and analytics. I\'ve designed and managed systems that handle large datasets, focusing on high availability, data integrity, and optimizing data flows to power business-critical applications, machine learning models, and reporting systems. Keeping these pipelines reliable, accurate, and high-performing is always top of mind, with continuous monitoring to ensure uptime and performance.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Data Pipelines</h4>\n      <p class="leading-relaxed mb-6">\n        I build data pipelines using tools like Apache Kafka, Airflow, and Spark to handle both real-time and batch data processing. My main goal is to ensure that data flows smoothly from source to destination with minimal latency and maximum accuracy. Whether it\'s supporting analytics, machine learning workflows, or day-to-day business operations, I make sure the pipelines are efficient and reliable. Monitoring and alerting are essential to keeping the systems running smoothly and ensuring real-time data is processed without bottlenecks.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Data Warehousing</h4>\n      <p class="leading-relaxed mb-6">\n        I have hands-on experience designing and managing data warehouses using platforms like AWS Redshift and Azure Synapse. My work involves creating efficient schemas, optimizing queries for performance, and ensuring the data is structured to support both transactional and analytical workloads. I keep a close eye on performance metrics and usage patterns, making sure that queries are running efficiently and that data storage is cost-optimized for the long term.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Data Governance & Quality</h4>\n      <p class="leading-relaxed mb-6">\n        Maintaining trust in data-driven decisions starts with data quality and governance. I implement data governance frameworks to ensure that data is secure, high-quality, and accessible throughout the pipeline. This includes defining data standards, applying access controls, and setting up quality checks to catch issues early. Continuous monitoring is essential to ensure compliance with governance policies and to maintain data integrity at every stage of the process.\n      </p>\n    '},{id:"security-modal",category:"Security",iconName:"fa6-solid:shield-halved",content:'\n      <p class="leading-relaxed mb-6">\n        Security has always been in the fabric of my work, whether I\'m building cloud software, deploying machine learning models, or managing media and data engineering pipelines. I\'ve designed and implemented security strategies that protect sensitive data across various systems, ensuring compliance with industry standards like HIPAA and PCI-DSS. From configuring secure cloud architectures to managing vulnerability assessments and incident response, my goal is always to keep systems secure, reliable, and compliant. Continuous security monitoring has been key to identifying and responding to potential threats proactively.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Application Security</h4>\n      <p class="leading-relaxed mb-6">\n        Ensuring security is embedded in the software development process is a key part of my work, starting from the earliest stages of building software. I incorporate security best practices into the codebase, such as secure coding standards, dependency management, and continuous vulnerability assessment. A key aspect of this is generating and maintaining a Software Bill of Materials (SBOM) to ensure full visibility into the open-source components and dependencies used within the software. This helps identify potential risks, such as outdated or vulnerable libraries, before they reach production.\n      </p>\n      <p class="leading-relaxed mb-6">\n        I also use security in the CI/CD process to ensure that code is thoroughly tested and vetted before deployment. Throughout the pipeline, I use tools like Snyk and Veracode to ensure that security checks are automated and performed early in the development cycle.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Cloud Security</h4>\n      <p class="leading-relaxed mb-6">\n        Securing cloud environments is one of the core aspects of my work. I focus on practices like role-based access control (RBAC), encryption (at-rest, in-use, and in-transit), and multi-factor authentication (MFA) to protect resources in AWS and Azure environments. I\'ve implemented security measures like IAM roles, key management systems, and secure networking configurations, ensuring that cloud resources are both secure and compliant with industry regulations. Using DataDog and AWS CloudTrail, I\'m able to detect potential security incidents early and respond quickly to mitigate them.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Network Security</h4>\n      <p class="leading-relaxed mb-6">\n        In terms of network security, I\'ve designed secure network architectures using VPCs, firewalls, and VPNs to ensure data remains protected in transit. My focus has been on zero-trust network principles, where no entity is trusted by default. By setting up continuous monitoring, I ensure that any threats are detected early and mitigated before they escalate. Whether it\'s securing real-time video streams or ensuring data pipelines are protected, network security plays a critical role in my work across various domains.\n      </p>\n      <h4 class="font-bold text-lg mb-3">Vulnerability Management & Compliance</h4>\n      <p class="leading-relaxed mb-6">\n        Managing vulnerabilities and ensuring compliance with standards like NIST, FedRAMP, and SOC 2 has been a big part of my work, especially in regulated industries like government, finance, and healthcare. I\'ve used tools like Snyk, Veracode, and AWS Inspector to identify and remediate security risks in both cloud-based applications and machine learning pipelines. Continuous vulnerability scanning and real-time monitoring have been essential for keeping systems secure and compliant, ensuring rapid response to potential threats as they arise.\n      </p>\n    '}],w=()=>{r.value=!1},k=()=>{null!==i.value&&(i.value=(i.value+1)%v.length)},x=()=>{null!==i.value&&(i.value=(i.value-1+v.length)%v.length)},I=()=>{y.value=!y.value,f.value+=1};return(e,u)=>(t(),n("div",T,[u[0]||(u[0]=a("h2",{class:"text-2xl font-semibold mb-6 text-center"},"Skills",-1)),a("div",z,[(t(),n(l,null,d(v,((e,n)=>a("div",{key:e.id,class:"flex flex-col items-center text-center"},[p(m(g),{icon:e.iconName,class:"text-4xl text-info mb-4","aria-hidden":"true"},null,8,["icon"]),a("h3",P,o(e.category),1),a("button",{class:"btn btn-sm mt-4",onClick:()=>(e=>{i.value=e,r.value=!0})(n)}," Learn More ",8,F)]))),64))]),null!==i.value?(t(),c(b,{key:0,"is-open":r.value,"on-close":w,title:v[i.value].category,"on-next":k,"on-previous":x,"total-pages":v.length,"current-page":i.value,"is-expanded":y.value,"on-toggle-expand":I,"trigger-overflow-check":f.value},{default:h((()=>[a("div",{innerHTML:v[i.value].content},null,8,L)])),_:1},8,["is-open","title","total-pages","current-page","is-expanded","trigger-overflow-check"])):s("",!0)]))}}),q={class:"flex flex-col items-center justify-center w-full p-6",role:"main","aria-label":"About Me Section"},N=e({__name:"About",setup:e=>(e,a)=>(t(),n("main",q,[p(f),p(W),p(A)]))});export{N as default};
//# sourceMappingURL=About-CPYbw_Sk.js.map
